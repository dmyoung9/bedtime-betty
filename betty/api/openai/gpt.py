import ast
import json
from typing import AsyncGenerator, Iterable, Literal, Optional, TypedDict

import openai
import tiktoken


from .. import BaseAPI
from . import guard_errors
from betty.prompt import Prompt

MODEL = "gpt-3.5-turbo-0301"
Role = Literal["system", "assistant", "user"]


class Message(TypedDict):
    role: Role
    content: str


def count_tokens(text: str, model: str = MODEL) -> int:
    """Returns the number of tokens in the given text,
    according to the specified model's tokenization rules."""

    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))


def system(content: str) -> Message:
    """Creates a system message with the given content."""

    return CompletionAPI.message("system", content)


def assistant(content: str) -> Message:
    """Creates an assistant message with the given content."""

    return CompletionAPI.message("assistant", content)


def user(content: str) -> Message:
    """Creates a user message with the given content."""

    return CompletionAPI.message("user", content)


class CompletionAPI(BaseAPI):
    """A class that provides methods for interacting with the OpenAI API."""

    def __init__(self, api_key: str):
        openai.api_key = api_key

    @staticmethod
    def message(role: Role, content: str) -> Message:
        """Creates a message with the given role and content."""

        return {"role": role, "content": content}

    def build_messages(
        self,
        prompt_filename: str,
        system_prompt_filename: Optional[str] = None,
        **kwargs,
    ) -> Iterable[Message]:
        prompt = Prompt.from_file(prompt_filename).format(kwargs)
        messages = [user(prompt)]

        if system_prompt_filename:
            system_prompt = Prompt.from_file(system_prompt_filename).format()
            messages.insert(0, system(system_prompt))

        return messages

    @guard_errors
    async def _get_completion(
        self,
        messages: Iterable[Message],
        model: str = MODEL,
        temperature: float = 1,
    ) -> str:
        """Returns the text generated by the model based on the given messages."""

        response = await openai.ChatCompletion.acreate(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=False,
        )

        return response["choices"][0]["message"]["content"]

    @guard_errors
    async def _stream_completion(
        self,
        messages: Iterable[Message],
        model: str = MODEL,
        temperature: float = 1,
    ) -> AsyncGenerator[str, None]:
        """Streams the generated text in chunks based on the given messages."""

        response = await openai.ChatCompletion.acreate(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True,
        )

        async for chunk in response:
            yield chunk["choices"][0]["delta"]

    async def get_json(
        self,
        messages: Iterable[Message],
        model: str = MODEL,
        temperature: float = 1,
    ) -> list[dict]:
        """Streams the generated text in chunks based on the given messages."""

        response = await self._get_completion(
            model=model,
            messages=messages,
            temperature=temperature,
        )

        return json.loads(response)
        # try:
        #     try:
        #         return json.loads(response)
        #     except json.JSONDecodeError:
        #         return ast.literal_eval(response)
        #     except Exception as e:
        #         raise ValueError("Response is not well-formed.") from e
        # except ValueError:
        #     json_match = re.findall(r"(?:```)(?:[a-z]*\n)?(.*?)(?:```)", response)
        #     try:
        #         return json.loads(json_match[0])
        #     except json.JSONDecodeError:
        #         return ast.literal_eval(json_match[0])
        #     except Exception as e:
        #         raise ValueError("Response is not well-formed.") from e

    async def stream_json(
        self,
        messages: Iterable[Message],
        model: str = MODEL,
        temperature: float = 1,
    ) -> AsyncGenerator[dict, None]:
        """Streams the generated text in chunks based on the given messages."""

        response = self._stream_completion(
            model=model,
            messages=messages,
            temperature=temperature,
        )

        buffer = ""
        start_position = -1
        brace_count = 0

        async for chunk in response:
            for char in chunk.get("content", ""):
                buffer += char
                if char == "{":
                    if start_position == -1:
                        start_position = len(buffer) - 1
                    brace_count += 1
                elif char == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        try:
                            yield json.loads(buffer[start_position:])
                        except json.JSONDecodeError:
                            yield ast.literal_eval(buffer[start_position:])

                        start_position = -1
                        buffer = buffer[len(buffer) :]  # Keep the buffer clean
